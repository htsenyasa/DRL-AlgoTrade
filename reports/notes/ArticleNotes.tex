\documentclass[twocolumn,aps,pra,superscriptaddress,nofootinbib,longbibliography]{revtex4-2}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[latin5]{inputenc}% For Turkish characters
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsbsy}
%\usepackage{mathptmx}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{physics}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{oubraces}
\usepackage{dsfont}
\usepackage[title, titletoc]{appendix}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage[most]{tcolorbox}
\usepackage{enumerate}
\usepackage{subfloat}
%\usepackage{hyperref}% add hypertext capabilities
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}%
\usepackage{cleveref}
\usepackage[normalem]{ulem}
\Crefname{subfigures}{figure}{figures}
\Crefname{subfigures}{Figure}{Figures}


\newcommand{\tcr}[1]{{\color{red} #1}}
\newcommand{\tcb}[1]{{\color{blue} #1}}
\newcommand{\tcg}[1]{{\color{gray} #1}}
\newcommand{\vari}[1]{\text{Var}(#1)}


\def\bra#1{\mathinner{\langle{#1}|}}
\def\ket#1{\mathinner{|{#1}\rangle}}
%\def\braket#1{\langle{#1}|\rangle}
\def\Bra#1{\left<#1\right|}
\def\Ket#1{\left|#1\right>}
%\newcommand{\dd}{\differential}

%\newcommand{\nc}{\newcommand}
%\nc{\rnc}{\renewcommand}
%\nc{\beg}{\begin{equation}}
%\nc{\eeq}{{\end{equation}}}
%\nc{\beqa}{\begin{eqnarray}}
%\nc{\eeqa}{\end{eqnarray}}
%\nc{\lbar}[1]{\overline{#1}}
%\nc{\bra}[1]{\langle#1|}
%\nc{\ket}[1]{|#1\rangle}
%\nc{\ketbra}[2]{|#1\rangle\!\langle#2|}
%\nc{\braket}[2]{\langle#1|#2\rangle}
%\newcommand{\braandket}[3]{\langle #1|#2|#3\rangle}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newenvironment{proof1}{\noindent\textbf{Proof:}}{\hfill \( \blacksquare \) \newline}



%***=============================================================***%
%***=============================================================***%


\begin{document}

\title{Deep Learning Project Progress Report}

\author{H\"useyin Talha \c{S}enya\c{s}a}
\email{senyasa@itu.edu.tr}
\affiliation{Department of Physics, Faculty of Science and Letters, Istanbul Technical University, 34469 Maslak, Istanbul, Turkey}




%\date{\today}

%***=============================================================***%
%***=============================================================***%

\begin{abstract}
My personal notes on \textit{An application of deep reinforcement learning to algorithmic trading}.

\end{abstract}

\maketitle

%***=============================================================***%
%***=============================================================***%




\section{Implementation of the Framework}

Some definitions and symbols

\begin{enumerate}
    \item \(S\): Set of environment and agent state.
    \item \(A\): Set of actions which are available for agent to use.
    \item \(s_t\): RL environment internal state
    \item \(o_t\): Observation
    \item \(a_t\): Trading action 
    \item \(i_t\): Information
    \item \(\pi(a_t|i_t)\): Trading policy (Rule)
    \item \(r_t\): Network's reward.
    \item \(\nu_{t}^c\): Total amount of cash in portfolio. 
    \item \(\nu_{t}^s\): Corresponding value of the share.
    \item \(n_t\): Total number of shares, lots. 
\end{enumerate}

Reinforcement learning techniques are concerned with the design of \(\pi\) maximizing an optimality criterion, which directly depends on the immediate rewards \(r_t\) observed over a certain time horizon.  

\subsection{Trading Environment}

The nature of an environment in deep reinforcement learning applications is dictated by the problem in question. In our case, the medium of the environment consists of discrete time series indexed by time-step \(t\). The interval, between sequential data points (corresponding to \(t-1\) and \(t\)) is determined by the trading frequency and it is denoted by \(\Delta t\). Since we only consider daily trading, the interval \(\Delta t\) is equal to 1 day. 

The information that is available to the agent for observation consists of two parts. The first part is the classical OHLCV (\textit{Open, High, Low, Close, Volume}) data of the stock in question, and the second part is the trading position which is denoted by \(P\). 
Formally the observation made by the agent at time-step \(t\) can be expressed as 
\begin{equation}
    \mathcal{O}(t) = \{p_t^O, p_t^H, p_t^L, p_t^C, V_t, P_t\},
\end{equation}
where 
\begin{enumerate}[-]
    \item \(p_t^O\) is the opening price of the stock over the period \([t - \Delta t, t]\),
    \item \(p_t^H\) is the highest price over the period \([t - \Delta t, t]\),
    \item \(p_t^H\) is the lowest price over the period \([t - \Delta t, t]\),
    \item \(p_t^C\) is the closing price over the period \([t - \Delta t, t]\),
    \item \(V_t\) is the total volume of shares exchange over the period \([t - \Delta t, t]\).
\end{enumerate}




Initialization, iteration and reset. In the initialization part, one first builds the environment in a state which can be determined randomly. This initialized state corresponds to current status of the environment. In the iteration part, first, the RL agent provides an \textit{action} to take, then, the environment state is transitioned to the next state according to the action provided by the RL agent.





% \begin{equation}
%     \begin{aligned}
%         \mathcal{E}_{TE} = \{&\text{Close, Low, High, Volume, Position,} \\
%         &\text{Action, Holdings, Cash, Money, Returns}\}
%     \end{aligned}
% \end{equation}

 


\subsubsection{Description of Trading Operations / Action Space}
The agent starts its trading activity with an initial \(C_0\) amount of money and it is chosen in such a way that it is much lower than the average exchange volume of the stock in question. In this way, it becomes safe to assume that the actions carried out by the agent does not influence the stock movements. 

For a given time-step \(t\), the agent can take two actions: \textit{long} and \textit{short}. A long action results in purchasing shares of the stock while a short action results in selling shares of the stock in question. 

Let \(C_t\) be the available cash at time-step \(t\) and let \(Q_t\) be the number of shares the agent is going to buy or sell. If the agent takes the long position at time-step \(t\), the agent will have 
\begin{equation}
    Q_t = \floor*{\dfrac{C_{t}}{p_t^C (1 + T)}},
\end{equation} 


\subsubsection{Implementation of Trading Operations}



One first needs to implement the trading operations to be able to implement the trading environment. The trading operations correspond to the actions that is taken by the agent. In normal circumstances, there must be a match between bid and ask orders to realize a trade operation. In order to detect these matches, one must be able access the order book of the stock in question. However, since assume that the total number of buy and sell orders are much smaller than the volume of the stock, we are going to implement only buy and sell operations. 

The trading operations are implemented in an object oriented manner. There are two main classes to control trading operations: \textit{StockHandler} and \textit{DummyPosition}. The StockHandler class obtains the stock data and applies minor transformations to eliminate possible missing data points. This class also responsible for the real-time tracking of the stock via \textit{Update} method. The DummyPosition is class is implemented to carry out possible actions i.e., \textit{long} or \textit{short} operations. and to keep track of the position changes. Here we provide the prototypes of the two classes. 


\subsubsection{Implementation of Trading Environment via OpenAI Gym}

The environment which the deep learning agent interacts with is implemented by utilizing \textit{OpenAI Gym} framework. OpenAI Gym is an open source python library that provides standardized application programming interface (API) to establish interaction/communication between agents and environments for reinforcement learning problems. 

A third party application has to implement/override several functions/methods that are inherited from the \textit{gym} base class. These methods include initialization, iteration and reset of the environment in question. 



\subsection{Implementation of Deep Neural Network}

Network Architecture, loss function etc.

\subsection{Implementation of Double-Q Mechanism}

\section{Additional notes and questions}

\begin{enumerate}
    \item Representing the transition from one candle to the next one as a Markov process.
    \item Considering the correlation between candles as a spin system (as in the case of Witten's ``An introduction to quantum information theory'') 
\end{enumerate}




\clearpage


\bibliography{biblio}

\end{document}
